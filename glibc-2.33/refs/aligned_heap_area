<def f='codebrowser/malloc/arena.c' l='497' type='char *'/>
<use f='codebrowser/malloc/arena.c' l='525' u='r' c='new_heap'/>
<use f='codebrowser/malloc/arena.c' l='527' u='r' c='new_heap'/>
<use f='codebrowser/malloc/arena.c' l='529' u='w' c='new_heap'/>
<use f='codebrowser/malloc/arena.c' l='547' u='w' c='new_heap'/>
<use f='codebrowser/malloc/arena.c' l='640' u='r' c='heap_trim'/>
<use f='codebrowser/malloc/arena.c' l='641' u='w' c='heap_trim'/>
<doc f='codebrowser/malloc/arena.c' l='489'>/* If consecutive mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...) calls return decreasing
   addresses as opposed to increasing, new_heap would badly fragment the
   address space.  In that case remember the second HEAP_MAX_SIZE part
   aligned to HEAP_MAX_SIZE from last mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...)
   call (if it is already aligned) and try to reuse it next time.  We need
   no locking for it, as kernel ensures the atomicity for us - worst case
   we&apos;ll call mmap (addr, HEAP_MAX_SIZE, ...) for some value of addr in
   multiple threads, but only one will succeed.  */</doc>
